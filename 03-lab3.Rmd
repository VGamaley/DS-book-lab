# Лабораторна робота №3. Розвідувальний аналіз даних. Візуалізація {#lab_3}


__Мета:__ _Засвоєння принципів, знайомство з інструментами та набуття навичок експлораторного (розвідувального) аналізу даних засобами мови програмування R та колекції пакетів `dplyr`, `ggplot2`, `desctable`, `"GGally", "corrplot", "PerformanceAnalytics", "FactoMineR", "factoextra"`, `"funModeling", "desctable", "ade4", "psych", "smacof", "WVPlots", "caret", "car"`._ 


## Що ви будете вміти?

* розв'язувати задачі розвідувального аналізу даних засобами мови R у середовищі IDE RStudio.
* візуалізувати результати аналізу.



## Короткі теоретичні відомості

### Що таке розвідувальний аналіз даних?

У рамках життєвого циклу процесу Data Mining згідно з методологією CRISP DM [@CRISP_DM], першою фазою аналізу даних є "Розуміння даних" (__Understanding__)  (рис. 1). До цієї фази входять розглянуті у попередніх лабораторних роботах задачі збору та різного роду трасформації. Наступною важливою задачею даної фази є вивчення даних, що складає основу так званого [_розвідувального аналізу даних_](https://en.wikipedia.org/wiki/Exploratory_data_analysis) (Exploratory data analysis, EDA).  

__Розвідувальний аналіз даних__ -- аналіз основних властивостей даних, виявлення в них загальних закономірностей, розподілів та аномалій, побудова початкових моделей шляхом їх перетворення та/або представлення у зручному вигляді: графічному, табличному, схем, діаграм і т.ін. [@EDA]    

![_Рис. 1. Структура задачі розуміння даних у складі Data Science-проекту_ [@r4ds] ](image/wrangling.png)  

Термін EDA був введений математиком Джоном Тьюкі, який сформулював цілі РАД таким чином:

* максимальне "проникнення" в дані
* виявлення основних структур
* вибір найважливіших змінних
* виявлення відхилень та аномалій
* перевірка основних гіпотез
* розробка початкових моделей

До основних інструментів РАД відносятся:

* аналіз ймовірностних розподілів змінних
* побудова та аналіз кореляційних матриць
* методи зниження розмірності даних: факторний аналіз, дискримінантний аналіз, багатовимірне шкалювання та ін.

РАД -- це ітеративна процедура, в результаті якої ми:

* формулюємо запитання щодо наших даних
* шукаємо відповідь за допомогою візуалізації, трасформації та моделювання наявних даних
* аналізуємо те, що отримано в результаті аналізу та формулюємо нові запитання.

"РАД є важливою частиною будь-якого аналізу даних, навіть якщо питання надається вам на блюді, тому що вам завжди потрібно вивчити якість ваших даних. Очищення даних -- це лише одне застосування РАД: ви задаєте питання про те, чи відповідають ваші дані вашим очікуванням чи ні." [@r4ds].  
Для очищення даних нам потрібно буде розгорнути всі інструменти РАД: візуалізацію, перетворення та моделювання.

### Питання

Не існує правил, яким ми могли би керуватися, в частині того, які питання нам слід задавати в процесі своїх досліджень. Проте два типи питань завжди будуть корисними для того, щоб робити відкриття у наших даних. У самій довільній формі ці питання можна сформулювати так [@r4ds]:

* Який тип варіації має місце в моїх змінних?
* Який тип коваріації має місце між моїми змінними?


#### Варіація

__Варіація (Variation)__ -- це тенденція до змін значення змінної від вимірювання до вимірювання. Ми можете легко помітити варіації в реальному житті; якщо ми двічі вимірюємо постійну змінну, ми отримаємо два різні результати.  
Категоричні змінні також можуть відрізнятися, якщо виміри робити для різних суб'єктів (наприклад, кольори очей різних людей) або у різні моменти часу (наприклад, енергетичні рівні електрона в різні моменти). Кожна змінна має свій власний паттерн у варіації, який може виявити цікаву інформацію. Найкращий спосіб зрозуміти цю закономірність -- візуалізувати розподіл значень змінної.  


##### Візуалізація розподілу

Нижче наведено приклад розподілу категоріальної змінної.

```{r echo=TRUE, message=FALSE, paged.print=FALSE}
library(tidyverse)
library(ggplot2)
ggplot(data = diamonds) +
  geom_bar(mapping = aes(x = cut))
```

Частоту для кожного значення категоріальної змінної можна обчислита, наприклад, так:
```{r}
diamonds %>% 
  count(cut)
```

Для неперервної змінної доцільно побудувати гістограму:

```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = carat), binwidth = 0.5)
```

Інтервальна таблиця частот, що відповідає гістограмі, може бути обчислена так:

```{r}
diamonds %>% 
  count(cut_width(carat, 0.5)) 
```

Можна побудувати шістограму для певної долі значень:

```{r}
smaller <- diamonds %>% 
  filter(carat < 3)
  
ggplot(data = smaller, mapping = aes(x = carat)) +
  geom_histogram(binwidth = 0.1)
```

Часто буває доцільно поудвати серія гістограм для різних груп спостережень:

```{r}
ggplot(data = smaller, mapping = aes(x = carat, colour = cut)) +
  geom_freqpoly(binwidth = 0.1)
```

Після того, як ми виконали візуалізацію, що ми маємо знайти на цих графіках? Яка може бути послідовність запитань на наступному етапі?  

*Типові запитання з урахуванням специфіка даної задачі можуть виглядати так:

* Які значення є найбільш поширеними? Чому?
* Які значення є рідкісними? Чому? Це відповідає нашим очікуванням?
* Чи бачемо ми якісь незвичайні закономірності? Що може їх пояснити?

Як приклад, гістограма нижче наводить кілька цікавих питань:

* Чому там більше діамантів праворуч від кожного піка, ніж трохи ліворуч від кожного піка?
* Чому немає діамантів більше 3 каратів?

```{r}
ggplot(data = smaller, mapping = aes(x = carat)) +
  geom_histogram(binwidth = 0.01)
```

##### Незвичайні значення

Як правило у вибіркових даних зустірчаються викиди (outliers) -- такі значення свідчать або про похибку вимірювання, або про якість надзвичайні причини, що потребують уважного вивчення.

```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5)
```


Для того, щоб їх побачити, необхідно певним чином масштабувати гістограму:

```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +
  coord_cartesian(ylim = c(0, 50))
```

Якщо приймається рішення їх видалити, то це можна зроити наступним чином:

```{r}
unusual <- diamonds %>% 
  filter(y < 3 | y > 20) %>% 
  select(price, x, y, z) %>%
  arrange(y)
unusual
```


##### Пропущені значення (Missing values)

Часто на практиці дані виявляються некомплектними -- мають місце пропущенні дані (`NA`). У таких випадках відомі два виходи з систуації:

* видалити некомплектні спостереження
* виконати імпутацію пропущених значень -- замінити пропущені значення певними у відповідності з якимось алгоритмом.

Пакет `ggplot2` автоматично видаляє некмоплектні дані: 

```{r}
diamonds
diamonds2 <- diamonds %>% 
  mutate(y = ifelse(y < 3 | y > 20, NA, y))
```

```{r}
ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + 
  geom_point()
```

Інколи ми хочемо зрозуміти, що робить спостереження з відсутніми значеннями, відмінними від спостережень із записаними значеннями. Наприклад, у `nycflights13::flights`, відсутні значення в змінній `dep_time` (час вильоту) показують, що рейс був скасований. Тому, можливо, нам потрібно буде порівняти заплановані терміни вильоту для скасованих та не скасованих часів. Ми можемо зробити це, зробивши нову змінну з `is.na()`:

```{r}
nycflights13::flights %>% 
  mutate(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %>% 
  ggplot(mapping = aes(sched_dep_time)) + 
    geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4)
```


#### Коваріація

Якщо варіація описує поведінку в межах змінної, коваріація описує поведінку між змінними.  
__Коваріація (Covariation)__ -- це схильність значень двох чи більше змінних змінюватися разом. Найкращим способом виявлення коваріації є візуалізація відносин між двома чи більше змінних.  

##### Категоріальні та неперервні змінні

Природнім є рішення щодо вивчення розподілу неперервної змінної, розбивши її на групи у відповідності до значень категоріальної змінної, як у попередньому багатокутнику частот. Поява за замовчуванням `geom_freqpoly()` не є таким корисним для подібного порівняння, оскільки його висоту визначає кількість. Це означає, що якщо одна з груп є набагато меншою, ніж інші, важко побачити відмінності у формі. Наприклад, давайте розглянемо, як ціна діамантів змінюється залежно від його якості:

```{r}
ggplot(data = diamonds, mapping = aes(x = price)) + 
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
```

Важко побачити різницю в розподілах, оскільки кількість вибіркових значень у кожній групі суттєво відрізняється:

```{r}
ggplot(diamonds) + 
  geom_bar(mapping = aes(x = cut))
```

Для полегшення порівняння нам потрібно поміняти те, що відображається на осі `Y`. Замість того, щоб відображати частоту, ми покажемо відносну частоту,  яка є нормованою величиною.

```{r}
ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + 
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
```

В результаті можна побачити, що найвищу середню ціну мають посередні діаманти (fair). Але аналіз щільностей розпоілів є не зовсім зручним. Альтернативним варіантом представлення аналогічної інформації є п'ятиквантильний графік (boxplot, box and wiskers plot), відомий як "боксплот", або "ящик з вусами". Боксплот акумулює в собі всі найважливіші інтегральні харакетристики стосовно мір центральної тенденції, розсіювання та форми розподілу (рис. 3).  

![_Рис. 3. Структура боксплота_ [@r4ds] ](image/boxplot.png)

Тоді для нашого випадку застосування боксплотів дасть такий результат:

```{r}
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
  geom_boxplot()
```

##### Дві категоріальні змінні

Для візуалізації коваріації між категоріальними змінними неохідно візуалізувати частоти: у вигляді таблиці, або певного графічного візуалізатора. Наприклад:


```{r}
ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color))
```

З результатів видно, що існує певна залежність між кольором діаманта, та якістю його ограненості.  


#### Пакет `FunModelling`

Відносно недавно з'явився пакет під назвою `FunModelling` [@R-funModeling], в арсеналі якого є набір корисних інструментів, що суттєво спрощують деякі процедури розвідувального аналізу, особливо на самому початку, коли вивчається структура даних. Зокрема

* `df_status()` : структура набору даних для профілювання
* `describe()` : чисельне та категоріальне профілювання (кількісне)
* `freq()`: категоріальне профілювання (кількісне та графік).
* `profileing_num()`: профілювання для числових змінних (кількісний)
* `plot_num (дані)` : профілювання для числових змінних (графіки)



### Зниження розмірності даних

__[Зниження розмірності (Dimensionality reduction)](https://en.wikipedia.org/wiki/Dimensionality_reduction)__ -- процесс скорочення кількості випадкових змінних шляхом отримання гооловних змінних. Цей процес можно поділити _обирання ознак_ та _виділяння ознак_.  
_Обирання ознак_ -- це процес пошуку первісних змінних (факторів), що починаються в рамках фази розуміння даних із залученням експертів предметної галузі і з залученням всього арсеналу інстурментів маніпулювання даними, про що йшлося вище. (Маніпулятивні методики).    
_Проектування ознак_ -- це перетворення даних з багатовимірного простору у простір простір невеликої кількості вимірів. (Математичні методики). (Далі під зниженням розмірнонсті будем мати на увазі саме проектування ознак). Існує велика кількість лінійних і нелінійних методик зниження розмірності.  
Що дає зниження розмірності на практиці? В першу чергу спрощення представлення багатовимірних даних, їх візуалізацію, вирішення задач класифікації та регресії і, власне, краще розуміння процесів, що моделюються.  
Одним з фундаментальних лінійних методів зниження розмірності, що широко викорстовується на практиці, є [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) [@PCA].  


#### Постановка задачі аналізу  методом главних компонент (PCA) 

Припустимо [@PCA], що ми маємо випадковий вектор $X$:
$$
X=\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
$$

З коваріаційною матрицею:  
$$
var(X) = \Sigma = 
\begin{pmatrix}
\sigma_{1}^2 & \sigma_{12} & \ldots & \sigma_{1p}\\
\sigma_{21} & \sigma_{2}^2 & \ldots & \sigma_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
\sigma_{p1} & \sigma_{p2} & \ldots & \sigma_{p}^2
\end{pmatrix}
$$

Мета [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) [@PCA_b] полягає в пошуку $k$ лінійних комбінацій $p$ змінних $X$, що містять найбільшу дисперсію. Лінійна комбінація має насупний вигляд:

$Y_1 = a_{11} X_1 + a_{12} X_2 + \cdots + a_{1p} X_p$ 

$Y_2 = a_{21} X_1 + a_{22} X_2 + \cdots + a_{2p} X_p$

$\vdots$

$Y_k = a_{k1} X_1 + a_{k2} X_2 + \cdots + a_{kp} X_p$


При цьому $\sum\limits_{i=1}^pa_{1i}^2=1$ і т. д.

Дисперсия першої главної компоненти $var(Y_1)=a_1'\Sigma a_1$, де $\Sigma$ -- ковариаційна матриця.

Аналогічно обчислюється дисперсія другої і т. д. головних компонент.

У даній моделі вектори $a_i'=(a_{i1}, a_{i2},...,a_{ip})',\;i=\overline{1,p}$ представляють _власні вектори_ ковариаційної матриці $\Sigma$, тоді як дисперсія $i$-ої головної компоненти дорівнює _власному значенню_ матриці ковариацій:

$var(Y_i)=\lambda_i$.

_Загальна дисперсія_ вибірки дорівнює $\sum\limits_{i=1}^p\lambda_{i}.$  
Метод головних компонент (PCA) вирішуючи задачу зниження розмірності дозволяє одночасно вирішити задачу сегементації (кластеризації) -- тобто з'ясувати, чи є досліджувані дані однорідними, чи сегментовані на групи зі схожими ознаками. Відповідь на це питання є однією з головних задач експлораторного аналізу, що передує етапу побуви більш складних моделей класифікації, регресії чи моделей на основі асоціативних правил.    

В арсеналі R існує багато інструментів для реалізації зниження розмірності, зокрема PCA.  



## Приклад виконання індивідуального завдання

### Постановка задачі

Дано стандартний багатовимірний набір даних [Іриси Фішера (англ. Iris flower data set)](https://en.wikipedia.org/wiki/Iris_flower_data_set).  
Іриси Фішера складаються з даних про 150 вимірювань ірисів з трьох видів —- Iris setosa, Iris virginica і Iris versicolor, по 50 вимірювань на вид. Для кожного екземпляра вимірювалися чотири характеристики (в сантиметрах):

* Довжина зовнішньої частки оцвітини (англ. sepal length);
* Ширина зовнішньої частки оцвітини (англ. sepal width);
* Довжина внутрішньої частки оцвітини (англ. petal length);
* Ширина внутрішньої частки оцвітини (англ. petal width).

Для даного набору виконати розвідувальний аналіз даних.  
Необхідно дати відповідь на такі питання:  

* чи є дані комплектними? з'ясувати характер розподілу змінних.
* чи пов'язані між собою характеристики? якщо пов'язані, то які і як?
* чи існує проста структура, яка дозволяє описати дані об'єкти в просторі розмірністю менш ніж `4`? якщо існує -- побудувати таку модель.
* чи є всі три види ірисів однорідними у даному просторі ознак? якщо ні, то виконати сегментацію -- знайти групи з однорідними ознаками.  
Результати представити із застосуванням простих і зрозумілих візуалізаторів.  

<!-- Іриси Фішера (англ. Iris flower data set) — це багатовимірний набір даних для задачі класифікації, на прикладі якого англійський статистик та біолог Рональд Фішер в 1936 році продемонстрував роботу розробленого ним методу дискримінантного аналізу. Іноді його також називають ірисами Андерсона — через те, що дані були зібрані американським ботаніком Едгаром Андерсоном. Цей набір даних став класичним і часто використовується в літературі для ілюстрації роботи різних статистичних алгоритмів. -->

### Виконання завдання



```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Інсталюємо необхідні пакети
packageNeed <- c("knitr", "dplyr", "ggplot2", "devtools", "sparklyr",
                 "GGally", "corrplot", "PerformanceAnalytics", "FactoMineR",
                 "factoextra", "funModeling", "desctable", "ade4", "psych",
                 "smacof", "WVPlots", "caret", "car")
# install.packages(packageNeed)
lapply(packageNeed, library, character.only = TRUE)
```

Дані з досліджуваного набору мають наступний вигляд:
```{r}
iris %>% 
  head()
# iris %>%
#   desctable()
```


Обчислимо і дослідимо сумарні статистики.
```{r}
# Сводные выборочные характеристики
iris %>% 
  df_status() 
```
Що ми бачимо? 

* всі дані мають числову природу
* дані комплектні: відстуні пропущені значення
* відсутні нульові значення -- це знімає можливі проблеми при трансформації
* відсутні надвелики значення -- надвелики значення кожен раз потребують серйозної уваги і аналізу можливих причин, що їх викликали
* по всіх змінних дані мають варіацію по унікальним значенням одного порядку.  

Дослідимо закони розподілу кожної з чотирьох змінних.


```{r}
# Гистограмм по всем переменным
iris %>%
  plot_num()
```


```{r}
iris %>%
  profiling_num()
# dimnames(iris)
```

Що ми бачимо?  

* статистичні розподіли змінних `"Sepal.Length"`, `"Sepal.Width"` мають дзвоноподібну форму, наближену до нормального. Враховуючи, що значення оцінок асимметрії та ексцесу несуттєво відрізняються від нуля, в першому наближенні можна вважати дані розподіли нормальними. Про що це говорить і що це дає? По-перше, це говорить про те, що доля малих і великих даних врівноважують одна одну, по-друге, нормальність законів розподілу досліджуваних величин, або, принаймні, "натяк" на нормальність __завжди добре__, тому що класичними передумовами для коректної побудови великої кількості різного роду моделей вимагає від даних нормального закону розподілу, чи, принаймні, симетричності закону розподілу. В нашому випадку це є передумовою однорідного розподілу спостережень у просторі інформативних ознак, що є позитивним моментом при вирішенні задачі сегментації.  
* статистичні розподіли змінних `"Petal.Length"`, `"Petal.Width"` на відміну від двох інших, мають чітку бімодальну структуру, що гооврить про явно виражену неоднорідність даних і про те, що саме ці дві змінні є дискримінуючими у просторі досліджуваних ознак; це важливо для побудови задачі сегментації

Для відповіді на питання, чи пов'язані між собою змінні, застосуємо кореляційний аналіз. З урахуванням числової природи даних, для оцінки кореляції скористаємося коефіцієнтом кореляції Пірсона. Враховуючі багатомірний аналіз початкових даних, важливо вдало підібрати візуалізатор. Нижче запропоновано два з найбільш відомих і поширених.

```{r}
# Кореляція Пірсона
iris %>%
  select(-Species) %>%
     cor() %>%
  corrplot(order = "hclust", tl.col='black', tl.cex=.75) 
  # chart.Correlation(histogram=TRUE, pch=19)
```



```{r}
# pairs(iris[1:4], main="Edgar Anderson's Iris Data", font.main=4, pch=19)

iris %>%
  select(-Species) %>% 
  pairs( main="Edgar Anderson's Iris Data", font.main = 4, pch = 19, col = iris$Species)

# pairs(iris[1:4], main="Edgar Anderson's Iris Data", font.main = 4, pch = 19, col = iris$Species)
```


```{r}

df_iris <- iris %>% 
  select(-Species)
 # df_iris %>%
 #   correlation_table("")
 
df_iris %>%
    cor() %>%
  # head(11)
  knitr::kable(caption = "Таблица оценок коэффициентов корреляции") 
```

Що ми бачимо? 

* має місце сильний позитивний кореляційний зв'язок між змінною `Sepal.Length` та змінними  `Petal.Length`, `Petal.Width`; на кореляційних полях чітко видно наявність даної кореляції
* на кореляційних полях чітко видно наявність неоднородності даних -- дані чітко поділяються на гомогенні групи за змінною `Species`
* має місце слабкий від'ємний кореляційний зв'язок `Petal.Length` і `Sepal.Width`; дана кореляція є уявною в силу сегментованості даних по змінній `Species`: якщо уважно дослідити форму кореляційних полів для кожного значення данної змінної, то можна побачити, що всередині кожного сегменту має місце позитивна кореляція

Що це нам дає?  

* наявність високого ступеня кореляції дає можливість знизити розмірність даних і знайти просту структуру у просторі меншої розмірності 
* у просторі меншої розмірності можна можна виконати сегментацію даних.

Для зниження розмірності і одночасно сегментації даних скористаємося методом головних компонент (PCA).

```{r}
# PCA 
resPCA <- iris %>% 
  select(-Species) %>%
  PCA(ncp = 8, graph = FALSE)
```


```{r}
# власні значення та кумулятивний процент
eigenvalues <- as.data.frame(resPCA$eig)
cumVar <- round(eigenvalues$`cumulative percentage of variance`[length(eigenvalues$eigenvalue[eigenvalues$eigenvalue >= 0.9])], 2)
```

```{r}
knitr::kable(
  eigenvalues, 
  caption = "Власні значення (eigenvalues) і сумарний процент поясненої дисперсії"
)
```


```{r}
# 
fviz_screeplot(resPCA, addlabels = TRUE,  ncp=10)
```

Чщо ми бачимо?  
Ми маємо $p=$ `r length(eigenvalues$eigenvalu[eigenvalues$eigenvalue >= 0.9])` головних компонент, які пояснюють `r cumVar` % дисперсії. Це значить, що м маємо всього дві нові компоненти замість чотирьох і практично без втрати інформації можемо представити всі спостереження в системі двох координат на площині: перша компонента по осі `Х`, друга -- по осі Y (див. рис.).  
Проаналізуємо детально структуру двох перших компонент, виключивши решту незначимих (див. табл. і рис.).

```{r}
# Навантаження для двох перших головних компонент
knitr::kable(
  resPCA$var$coord[ ,1:2], 
  caption = "Таблиця навантажень"
)

```

Що ми бачимо?

* Структуру першої компоненти головним чином складають три початкові змінні: `Sepal.Length`,  `Petal.Length`, `Petal.Width`; як і прогнозувалося раніше, саме за цією компонентою відбувається дискримінація (розрізнення) трьох різних сегментів трьох типів ірисів
* основне навантаження на другу компоненту складає змінна `Sepal.Width` -- всі три види ірисів можуть мати досить велику варіацію за цим параметром.

```{r}
# Biplot of individuals and variables
fviz_pca_biplot(resPCA,
                geom = c("point"),
                # label = "none", # hide individual labels
             habillage = as.factor(iris$Species), # color by groups
             axes = c(1, 2),
             repel = TRUE,
             label = c("ind", "ind.sup", "quali", "var", "quanti.sup"),
             select.var = list(name = c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")),
             # select.var = list(contrib = 8),
             # label = c("ind.sup"),
             palette = c("#00AFBB", "#E7B800", "#FC4E07", "#00AFBB", "#E7B800", "#FC4E07"),
             # alpha.var = c("contrib"),
             # col.ind = c("contrib"),
             # col.ind.sup = c("contrib"),
             addEllipses = TRUE # Concentration ellipses
             ) +
  theme_minimal()

```

Таким чином, з'ясовано, шо початкові дані не є однорідними. Три типи ірисів різняться за довжинами внутрішніх часток оцвітини (petal length) та шириною внутрішньої частки оцвітини (petal width). Завдяки наявності кореляцій у початкових змінних, спостереження вдалося добре описати у просторі двох інтегральних показників. Знайдені кластери характерні для трьох типів ірисів і у майбутньому можуть бути використані для написання класифікатора з метою розпізнавання нових об'єктів.  



### Індивідуальні завдання на лабораторну роботу

Видає викладач.


