# Модуль 2. Моделювання. Лабораторна робота №5. Побудова регресійних моделей {#lab_5}


__Мета:__ _Засвоєння базових принципів, знайомство з інструментами та набуття навичок побудови моделей регресії_ __на рівні технології__ на основи статистичного підходу та моделей машинного навчання засобами мови програмування R та колекції пакетів `dplyr`, `ggplot2`. 


## Що ви будете вміти?

* будувати моделі парної і багатовимірної лінійної та нелінійної регресії на основі статистичних моделей та моделей машинного навчання засобами мови R у середовищі IDE RStudio.



## Короткі теоретичні відомості

У рамках життєвого циклу процесу Data Mining згідно з методологією CRISP DM [@CRISP_DM], наступною за  фазою "Підготовка даних" (__Data Preparing__) є фаза "Моделювання" (__Modelling__) (рис. 1).     

Фаза моделювання призначена для вибору оптимального методу побудови моделей і налаштування його параметрів для отримання оптимальних рішень. На даній фазі вирішуються наступні задачі:  
- вибір методу моделювання;  
- генерація тестового проекту;  
- створення моделей;  
- оцінка моделей.   

Згідно з класичним розумінням технологія Data Mining передбачає побудову моделей, які можна віднести до одного з п'яти основних класів: _кластеризація (сегментація) та аналіз відхилень, регресія, класифікація, пошук асоціативних правил та аналіз послідовних шаблонів_.  

![_Рис. 1. Задача моделювання у складі Data Science-проекту_ [@r4ds] ](image/data-science-model.png)  


Задача сегментації та аналізу відхилень розглядалася нами як складова розвідувального аналізу даних [лаб. роб. №3](#lab_3) у рамках базового модулю.  
У рамках даного модулю буде розглянуто решту моделей. Ця лабораторна робота присвячена регресійному аналізу. Побудова моделей класифікації розглядатиметься у [лабораторній роботі № 6](#lab_6).


### Що таке регресія і регресійний аналіз?

__Регресія і класифікація__

Між задачею класифікації і регресії існує багато спільного і в самому загальному вигляді їх можна розглядати як одну. Вона може бути сформульована наступим чином: припустимо, що об'єкт, який нас цікавить, описується вектором $n$ незалежних змінних $X_1, X_2, \dots, X_n$, які називаються _предикторами_. Існує деяка величина $Y$, яка також характеризує досліджуваний об'єкт, але залежить від $X_1, X_2, \dots, X_n$. Ми маємо колекцію наборів  спостережень незалежних змінних у вигляді матриці $X$ та залежної змінної у вигляді вектора відгуків $Y$: 

$$X=\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1,n} \\
x_{21} & x_{22} & \cdots & x_{2,n} \\
\vdots & \vdots & \ddots & \cdots \\
x_{n1} & x_{n2} & \cdots & x_{n,n} \\
\end{bmatrix},$$  
та 
$$Y=\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}.$$  

На основі інформації, яку несуть у собі наявні значення $X$ та $Y$ необхідно побудувати модель, яка буде передбачати значення відгуку $y^*$ для будь-якого заданого набору незалежних змінних $x^*_1, x^*_2, \dots, x^*_n$  

Відмінність полягає у наступному (рис. 1): якщо значення відгуку носять дискретний характер, говорять про задачу _класифікації_, якщо неперервний -- має місце задача _регресії_.

![Рис. 1. Ілюстрація задач класифікації і регресії [@Paklin]](image/regression_classification.png) 

Таким чином логічно припустити, що і з точки зору математичного апарату та алгоритмів, що його реалізують при розв'язанні задач регресії і класифікації, має бути багато спільного, але, звичайно, мають бути і відмінності.  

У цій лабораторні роботі розглядаються методи і засоби регресійного аналізу.


### Формальна постановка задачі регресії

З точки зору _параметричного підходу_ задача регресії полягає у побудові моделі функціональної залежності математичного сподівання відгуку $Y$ за допомогою невідомої функції регресії $f(\dots)$ з використанням навчальної вибірки:
$$E(Y|x_1, x_2, ..., x_n)=f(
\beta, x_1, x_2, ..., x_n) + \epsilon,$$
де залишки $\epsilon$ відображають похибку моделі, тобто непояснену випадкову варіацію спостережуваних значень залежної змінної відносно очікуваного середнього значення. Такий підхід називається _статистичним_.  

Однак далеко не завжди можливо підібрати адекватну функцію регресії за умови великої кількості предикторів та(або) за умови складного характеру самої регресійної залежності. Тому наразі добре вивчені і широко поширені моделі регресії на основі _машинного навчання_, які дозволяють розв'язувати цю проблему. 

У роботі [@Fernandes] автори дослідили широке коло існуючих моделей класифікації і зробили висновок стосовно чьотирьох класів моделей, які мають найбільшу точність:  
* __Випадковий ліс” (Random Forest)__;  
* __Машини опорних векторів (Support Vector Machines)__;  
* __Штучні нейронні мережі (Artificial Neural Networks)__;  
* __Бустінгові ансамблі моделей (Boosting Ensembles)__.  

Однак [автори](https://ranalytics.github.io/data-mining/01-Data-Mining-Models-in-R.html#section_1_1) слушно зауважують, що, "перелічені методи практично непридатні для інтерпретації механізмів явища, яке прогнозується, що викликало ряд критичних зауважень".  На підтвердження даної думки можна навести той [факт](https://r-analytics.blogspot.com/2019/09/enterprise-applications-of-r-language.html), що тема [моделей, які можуть бути пояснені (Explainable artificial intelligence, пояснимі моделі)](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence) наразі є найгорячишою темою дискусій у професійному середовищі аналітиків даних.  

Досвід показує, що в тих випадках, коли кількість предикторів є невеликою і залежність є незанадто складною, параметричні методи дозволяють побудувати адекватну модель з високою прогностичною силою, яка одночасно дозволяє і легку інтерпретацію поведінки об'єкта, що вивчається.  

Нижче розглядається статистичний підхід на основі _методу найменших кравдратів (МНК)_ і підхід на основі машинного навчання на прикладі _випадкових лісів._  


### Статистичний підхід

Будемо використовувати матричний підхід.  
Модель лінійної за параметрами регресії у матричній формі має вигляд:

$$Y=X\beta+\epsilon.$$

На практиці ми шукаємо оцінку цього рівнняння:

$$\hat{Y}=Xb,$$

де $b=\begin{bmatrix}
b_0 \\
b_1 \\
\vdots \\
b_k
\end{bmatrix}$. 

На практиці маємо матрицю спостережень предикторів $X$ та вектор відгуків $Y$. Введемо вектор залишків $\epsilon$: 

$$\epsilon=\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots \\
\epsilon_n\\
\end{bmatrix},$$

де $$\epsilon=Y - \hat{Y} = Y - Xb.$$
Відомо, що суть МНК полягає в отриманні оцінок вектора $b$ за умови мінімізації суми квадратів залишків:
$$U(b) = \epsilon^T \epsilon = (Y - Xb)^T (Y - Xb) \rightarrow min. $$
Виконавши диференціювання за параметром $b$ і прирівнявши похідну до нуля, отримаємо нормальні рівння:

$$X^TXb = X^TY,$$

розв'язок яких відносно параметра $b$ дає нам МНК-оцінку вектора коеффіцієнтів моделі $\beta$:

$$b=(X^TX)^{-1}X^TY.$$
 
__Приклад.__ Припустимо, що випадкова величина $X$ -- зріст дорослої людини (_см_), а $Y$ -- вага її тіла (_кг_). Ми виконали $n=4$ спостереженнь зросту і ваги випадкових перехожих і хочемо побудувати модель парної лінійної регресії $y=\beta_0 + \beta_1 + \epsilon.$ Для цьго необхідно знайти оцінку рівняння регресії $\hat y = b_0 + b_1x$, попередньо знайшовши МНК-оцінку вектора коефіцієнтів $b \sim \beta$. Запишемо наші дані у матричному вигляді:
$$X=\begin{bmatrix}
1 & 155 \\
1 & 198 \\
1 & 164 \\
1 & 178 \\
\end{bmatrix}, Y=\begin{bmatrix}
60 \\
101 \\
61 \\
85 \\
\end{bmatrix}.$$



Важливо зауважити, що для реалізації МНК у матричній формі матриця $X$ має бути модифікована шляхом додавання одиничного стовпчика зліва.  

Підставимо наші дані у формулу МНК-оцінки: 
$$
b=(X^TX)^{-1}X^TY= 
\Bigg( \begin{bmatrix}
1 & 1 & 1 & 1 \\
155 & 198 & 164 & 178 \\
\end{bmatrix}
\begin{bmatrix}
1 & 155 \\
1 & 198 \\
1 & 164 \\
1 & 178 \\
\end{bmatrix}\Bigg )^{-1}
\begin{bmatrix}
1 & 1 & 1 & 1 \\
155 & 198 & 164 & 178 \\
\end{bmatrix}
\begin{bmatrix}
60 \\
101 \\
61 \\
85 \\
\end{bmatrix} =
\begin{bmatrix}
-103.272\\
1.036
\end{bmatrix}
$$ 
 
Засобами пакета `Matrix` можна провести аналогічні обчислення і отримати аналогічний результат. 

```{r}
suppressMessages(library(Matrix))

X <- matrix(c(1, 155, 1, 198, 1, 164, 1, 178), nrow = 4, ncol = 2, byrow = TRUE)
Y <- c(60, 101, 61, 85)
b <- solve((t(X) %*% X)) %*% t(X) %*% Y
b

```
Побудуємо лінію регресії, скориставшись стандартними засобами мови R.

```{r}
plot(X[, 2], Y,
     main = "Лінія регресії",
     xlab = "x, см",
     ylab = "y, кг"
     )
abline(a = b[1], b = b[2], col = "blue")

```

З графіка видно, що лінія регресії адекватно відображає динаміку "хмари" точок. Однак наступним і важливим етапом є перевірка моделі на адекватність. У середовищі R існує велика кількість спеціалізованих засобів, які дозволяють реалізувати різні статистичні методи побудови моделей регресії, зокрема на основі МНК, і перевірити їх адекватність (див. [Приклад виконання індивідуального завдання](#ind)).  



### Підхід на основі машинного навчання


В якості прикладу моделі машинного навчання, придатної як для розв'язання як задач регресіє, так і класифікації, розгляном модель на основі випадкових лісів (Random Forest).  

Модель на основі випадкових лісів (Random Forest) [@Breiman2001] у модифікації Лео Бреймана та Адель Катлер ґрунтується на процедурі беґґінга у сполученні з методом випадкових підпросторів. При цьому в якості базових класифікаторів використовуються некорельовані дерева, що будуються за алгоритмом CART [@cart]. Метод випадкових підпросторів дозволяє знизити корельованість між деревами і уникнути перенавчання.

Алгоритм побудови ансамблю моделей, що використовують метод випадкового підпростору має наступний вигляд:
	
1. Нехай кількість об’єктів для навчання -- $N$, а кількість ознак -- $D$.
	
2. Виберемо L як кількість окремих моделей в ансамблі.
	
3. Для кожної окремої моделі $l$ виберемо $dl (dl<L)$ як кількість ознак для $l$. Як правило для всіх моделей використовується тільки одне значення $dl$.
	
4. Для кожної окремої моделі $l$ створюємо навчальну вибірку, обравши $dl$ ознак з $D$ і навчаємо модель.
	
5. Для обчислення прогнозного значення для нових даних усереднюємо результати окремих $L$ моделей.


Алгоритм побудови випадкового лісу для N дерев має наступний вигляд:

1. Для кожного $n = 1,..., N$:
 
2.	Згенерувати вибірку $X_n$ за допомогою бутстрепа.

3. Побудувати дерево $b_n$ за вибіркою $X_n$:

4. За заданим критерієм обираємо найкращу ознаку, виконуємо розбиття у дереві по ньому і так доти, поки вибірку не буде вичерпано.

5. Дерево будується, доки у кожному листі не більше ніж $n_{min}$, або доки не досягнемо певної висоти дерева.

6. При кожному розбитті спочатку обирається $m$ випадкових ознак з $n$ початкових, і пошук оптимального поділу вибірки виконується тільки серед них.
 
7. Підсумковий класифікатор $a(x) = \frac{1}{N}\sum_{i=1}^N b_i (x)$.
  
  
Випадковий ліс має високу точність прогнозу і нечутливий до викидів і неоднорідності початкових даних. Більше того, алгоритм дозволяє оцінити важливість первинних ознак, що буде показано у [прикладі](#rf) нижче.  

Однак в якості головних недоліків відмітимо складність інтерпретації моделі та великий їх розмір: необхідно $O(NK)$ пам’яті для зберігання моделі, де $K$ –- кількість дерев. Головним чином із-за останнього недоліку в якості гідної альтернативи пропонуються градієнтний бустінг для побудови ансамблю, який за точністю не поступається випадковим лісам, але має менший розмір моделі.


## Приклад виконання індивідуального завдання {#ind}

__Задача__: оцінити охоплення аудиторії у Facebook за відомими показниками likes/shares/comments

Покажемоприклад розв'язання, спираючись на стандарт CRISP DM [@CRISP_DM].

### Розуміння даних


__Первиниий збір даних__


Маємо [набір даних](data/data.xls), котрий містить вибірку об'єму $n=99$ статистичних даних залежності охоплення (розміру) аудиторії публічного акаунта і реакціями аудиторії на певні публікації (табл. 1).


__Опис даних__


Таблиця 1. __Структура початкових статистичних даних__

Характеристика  | Позначення, тип | Кодовое значення 
------------- | ------------- | -------------
номер публікації (первинний ключ)  | $№$, ціле | `posts`
кількість коментарів до даної публікаії  | $x_1$, ціле | `comments`
кількість лайків до даної публікації  | $x_2$, ціле | `likes`
кількість перепостів даної публікації  | $x_3$, ціле | `shares`
сумарна реакція  | $x_{123}=\sum_\limits{i=1}^3x_i$, ціле | `all reactions`
охоплення аудиторії  | $y$, ціле | `reach`

 
__Вивчення і перевірка якості даних__

 
```{r, include=FALSE}
# Інсталюємо необхідні пакети
x <- c("dplyr","rio", "ggplot2", "tidyr", "corrplot", "rio", "PerformanceAnalytics", "FactoMineR", "factoextra", "caret", "GGally", "randomForest")
# if (any(is.na(match(x,installed.packages())))) {
#   install.packages(x)
#   lapply(x, library, character.only = TRUE)
# } else {
#   lapply(x, library, character.only = TRUE)
# }

# install.packages(x)
lapply(x, library, character.only = TRUE)
```


```{r, include=FALSE}
# Импортируем данные
library(magrittr)
library(rio)
y <- c("data/data.xls")
data <- import("data/data.xls")
# View(data)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(
  head(data),
  caption = "Таблиця початкових даних"
)
```

Кількість даних __є малою для побудови адекватної прогнозної моделі__, однак, як буде показано нижче, зважаючи на наявність високої кореляції між предикторами і відгуком може бути запропонований до розгляду ряд моделей,які показують обнадійливі результати і на наявних статистичних даних і в перспективі можуть бути перенавчені на нових даних.


### Підготовка даних

 

Дані комплектні, не вимагають траснформаціі і дозволяють перейти безпосередньо до фази побудови моделі.


### Моделювання {#rf}


> "Не следует множить сущее без необходимости" (William of Ockham)



__Вибір методу моделювання__

Побудова моделі прогнозування охоплення аудиторії є задачею регресійного аналізу, яку в термінах прийнятих позначень (див. табл. 1) формально можна записати в такий спосіб: необхідно знайти оцінку функції (параметричних або непараметрическим способом) $y = f (x_1, x_2, x_3)$, що дозволяє прогнозувати значення кількісної змінної $y$ від набору незалежних змінних $x_1, x_2, x_3$.

З огляду на те, що незалежні змінні і відгук мають числову природу, доцільно в якості основи параметричного підходу взяти класичну статистичну модель багатовимірної лінійної регресії на основі методу найменших квадратів (МНК). Підстави -- легка інтерпретація коефіцієнтів моделі.

Водночас є підстави припускати, що в перспективі дані можуть мати сегментовану структуру, утворювати гомогенні групи, тому доцільно пошукати альтернативний варіант серед непараметричних моделей на основі машинного навчання. Наприклад, на основі нейромереж або random forest. Для такого роду ситуацій найкращим чином (за даними літературних джерел і, зокрема, особистого досвіду автора [@Slab]) підходить модель регресії на основі random forest, яка, на відміну, наприклад, від нейромереж і SVM-моделей добре працює __без попередньої сегментаціі__ вибіркових даних.

З урахуванням вищесказаного і на підставі результатів розвідувального аналізу даних для розв'язання задачі можна запропонувати до розгляду наступні моделі (в першому наближенні):

* Модель множинної лінійної регресії (multiple regression) на основі МНК (Ordinary Least Squares, OLS): $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3)$; у разі необхідності зниження розмірності - покрокова регресія або регресія на головні компоненти (Principal Component Regression);

* Модель парної (Simple regression) лінеарізуемой нелінійної регресії на основі МНК (Squared-Y): $y = \sqrt {\beta_0 + \beta_1x_2}$;

* модель парной лінеарзизуємої нелінійної регресії на основі МНК (Squared-Y): $y=\sqrt{\beta_0+\beta_1x_{123}}$;

* Модель множинної регресії $y = f (x_1, x_2, x_3)$ на основі випадкового лісу ($random \; forest$).


__Генерація тестового проекту, створення моделей та їх оценка__


Досліджуємо таблицю багатовимірних вибіркових даних, обчисливши оцінку коефіцієнта кореляції Пірсона і побудувавши кореляційні поля (див.нижче).

```{r}
data %>%
  select(comments:shares, `all reactions`, reach) %>% 
  cor() %>% 
  knitr::kable(caption = "Таблиця оцінок коефіцієнтів кореляції")
```


```{r}
data %>%
  select(comments:shares, reach) %>% 
ggpairs()
```

```{r, include=FALSE}
CorrMatrix <- data %>% 
  select(comments:shares, `all reactions`, reach) %>% 
  cor()
```


Що ми бачимо? 

1. Відгук `reach` має позитивну кореляцію середньої степені з кожною з трьох вхідних змінних:
    + `comments`: `r CorrMatrix[5, 1]`  
    + `likes`: `r CorrMatrix[5, 2]`
    + `shares`: `r CorrMatrix[5, 3]`
1. Всі три незалежні змінні мають середню і високу ступінь кореляції між собою, що говорить про наявність мультиколінеарності:
    + `comments`-`likes`: `r CorrMatrix[1, 2]`  
    + `comments`-`shares`: `r CorrMatrix[1, 3]`
    + `likes`-`shares`: `r CorrMatrix[2, 3]`

1. Розподілу незалежних змінних і відгуку мають позитивну асиметрію, про що говорить наявність правого хвоста.

1. Дані мають викиди (outliers), які одночасно є і впливовими точками (influential points), тобто виключення або включення цих точок у модель істотно впливає на її параметри.

Що це означає?

1. Є підстави вважати, що змінні `comments`,` likes`, `shares` впливають на охоплення аудиторії` reach` і можуть виступати в якості незалежних змінних при побудові прогнозної моделі. (Це добре)

1. Наявність мультиколінеарності говорить про те, що лінійна модель прогнозування $y=f(x_1, x_2, x_3)$ буде неадекватною, при цьому система незалежних змінних надлишкова і вимагає застосування процедур щодо зниження розмірності. (Це не дуже добре)

1. Позитивна асиметрія розподілів змінних викликана наявністю викидів вправо. Це ще один аргумент на сторону припущення щодо неадекватності лінійних моделей. Якщо таке трапиться, то вихід може бути, наприклад, таким - вирівнювання розподілів за допомогою логарифмічних або інших нелінійних функцій з подальшим застосуванням лінійного МНК (лінеаризація моделі). (Це не дуже добре)

1. Наявність впливових точок - серйозна проблема, особливо в нашому випадку - вибірка мала і тому при проведенні семплювання для перевірки стійкості оцінок коефіцієнтів моделей це може створювати проблеми. (Це погано) Тим не менше, є міркування, що деякі впливові точки доцільно включати в модель (див. Нижче).



__Модель множинної лінійної регресії (multiple regression) на основі МНК (Ordinary Least Squares, OLS): $y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3)$ з покроковою процедурою__



Побудуємо модель лінійної регресії  дя всіх точок. Видно, що точки з номерами $40$ і $94$ не вписуються в загальну картину, тому ми їх вилучаємо і проводимо повторну оцінку коефіцієнтів моделі.


```{r}
dataNotFilter <- select(data, comments:shares, reach)
lm.reach <- lm(reach ~ ., data = dataNotFilter)
summary(lm.reach)
plot(lm.reach)
```

Виключаємо викиди та повторно здійснюємо побудову моделі регресії.
 
```{r}
dataFilter <- data %>%
  filter(posts %in% c(40, 94) != TRUE) %>% 
  select(comments:shares, reach)

lm.reach <- lm(reach ~ ., data = dataFilter)
summary(lm.reach)
plot(lm.reach)
```

Результати регресійного аналізу показивають наступне:

1. Регресія має місце з коефіцієнтом детермінації $R^2=$ `r summary(lm.reach)$r.squared`. Тобто модель здатна пояснити варіативність відгуку на `r summary(lm.reach)$r.squared*100` $\%$, говорячи простими словами, модель " є гарною" на стільки ж процентів.

1. Всі коефіцієнти, окрім `likes` є незначимими, що говорить про наявність мультиколінеарності і про те, що з трьох незалежних змінних саме кількість лайків наибільше корелює з відгуком. Значимість вільного члена ($b_0$) на межі, що натякає на необхідність спроби  побудувати модель без константи.

Вихід -- зниження розмірності та повторна побудова моделі.

Для зниження розмірності моделі і, відповідно, усунення мультиколінеарності, скористаємося покроковою процедурою включення з вилученням слабких предикторів ( _Forward Stepwise Selection_ ).



```{r}
lmStep.reach <- step(lm.reach, trace = 0)
summary(lmStep.reach)
```

Видно, що у результаті покрокової регресії ми отримали просту модель парної регресії у вигляді $y=f(x_2)$ практично без зменшення її точності, що і підтверджує порівняльний дисперсійний аналіз двох моделей -- $y=b_0+b_1x_1+b_2x_2+b_3x_3$ та $y=b_0+b_1x_2$:

```{r}
anova(lm.reach, lmStep.reach)
```

Виконаємо тестування двох моделей з використанням десятикратної перехресної перевірки (cross validation).

```{r, warning=FALSE}
lm.reach.cv <- train(reach ~ ., data = dataFilter, method = 'lm', trainControl = trainControl(method = "cv"))
```

```{r, warning=FALSE}
lmStep.reach.cv <- train(reach ~ likes, data = dataFilter, method = 'lm', trainControl = trainControl(method = "cv"))
```


```{r}
lm.reach.cv
lmStep.reach.cv
```

Як видно з результатів, проста модель лінійної регресії має меншу похибку (RMSE), ніж модель множинної регресії, хоча спостерігається невелике зниження (на $2%$) коефіцієнта детермінації у другої моделі. Але, як свідчить проведений вище дисперсійний аналіз, це незначимо. Цю модель можна покращити, вилучивши константу зі специфікації моделі, тобто отримати модель у вигляді $y=b_1x_2$:


```{r}
lm.MinusConst.reach <- lm(reach ~ likes - 1, data = dataFilter)
summary(lm.MinusConst.reach)
plot(lm.MinusConst.reach)
```


```{r}
ggplot(dataFilter,
       aes(x = likes - 1, y = reach,
           colour = comments)) +
  labs(title = "Залежність охоплення аудиторії від кількості лайків",
       subtitle = "Лінійна регресія з 95% довірчими межами",
       caption = "Без коригування. Кольором виділено кількість коментарів", 
       x = "Кількість лайків", y = "Охоплення аудиторії, кільк. люд.") +
  geom_point() +
  stat_smooth(method=lm, se = TRUE, fullrange = TRUE) 

```



Виходячи з правила "трьох сигм", для коригування лінійної моделі доцільно видалення ще двох точок (11, 24).
  

```{r}
dataFilterThreeSigma <- data %>%
  filter(posts %in% c(11, 24, 40, 94) != TRUE) %>% # указываем номера точек, которые должны быть исключены
  select(comments:shares, reach)
```

Будуємо модель.

```{r}
lm.MinusConstThreeSigma.reach <- lm(reach ~ likes - 1, data = dataFilterThreeSigma)
summary(lm.MinusConstThreeSigma.reach)
# plot(lm.MinusConst.reach)
```

Будуємо графік.

```{r}
ggplot(dataFilterThreeSigma,
       aes(x = likes - 1, y = reach,
           colour = comments)) +
  labs(title = "Залежність охоплення аудиторії від кількості лайків",
       subtitle = "Лінійна регресія з 95% довірчими межами",
       caption = "З коригуванням. Кольором виділено кількість коментарів", 
       x = "Кількість лайків", y = "Охоплення аудиторії, кільк. люд.") +
  geom_point() +
  stat_smooth(method=lm, se = TRUE, fullrange = TRUE) 
```



Що маємо і як з цим працювати?

1. Рівняння моделі: $y=$ `r summary(lm.MinusConstThreeSigma.reach)$coefficients[1]` $\cdot x_2$
Маємо гранично просту і легко інтерпретовану модель: серед двох публікацій, у однієї з яких лише на один лайк більше, в средньому на `r floor(summary(lm.MinusConstThreeSigma.reach)$coefficients[1])` переглядів аудиторія більше.

1. Регресія має місце з коефіцієнтом детермінації $R^2=$ `r summary(lm.MinusConstThreeSigma.reach)$r.squared`. Тобто модель здатна пояснити варіацію відгуку на `r summary(lm.MinusConstThreeSigma.reach)$r.squared*100` $\%$, кажучи простими словами, модель "хороша" на стільки ж відсотків.

1. Сіра зона на графіку показує надійну зону регресії -- нижню та верхню $95\%$-у межу прогнозу для середньої кількості лайків. Обчислюється вона так:


```{r}
likesNumber <- data.frame(likes=c(200, 250, 255)) # вказуємо значення likes для обчислення прогнозу по reaches
pre <- predict(lm.MinusConstThreeSigma.reach, likesNumber, interval="confidence")
knitr::kable(cbind(likesNumber, pre),
             caption = "Точковий та інтервальний прогноз охоплення аудиторії")
```

Наприклад, якщо публікація набрала $200$ лайків, то модель з надійністю не гірше, ніж $95\%$ гарантує, що охоплення аудиторії в середньому `r floor(pre[1, 1])`, але не менше, ніж `r floor(pre[1, 2])`, але не більше, ніж `r floor(pre[1, 3])`.



__Щодо інших альетернативних моделей__ 


Щодо заявлених на початку альтернативних нелінійних моделей:

* модель парної (Simple regression) лінеаризованої нелінійної регресії на основі МНК (Squared-Y): $y=\sqrt{\beta_0+\beta_1x_2}$;

* модель парної лінеаризованої нелінійної регресії на основі МНК (Squared-Y): $y=\sqrt{\beta_0+\beta_1x_{123}}$;


Як показали дослідження, ці моделі несуттєво відрізняються за точністю від запропонованої лінійної моделі з одним предиктором `likes`, тому немає жодних підстав висувати їх як достойну альтернативу.


Адекватну прогнозну модель на основі випадкових лісів ($random\; forest$) навіть у робочому варіанті на малій вибірці менше 100 будувати недоцільно. При збільшенні даних хоча б на порядок і за наявності залежностей, відмінних від лінійної, цей підхід може дати цікаві та непогані результати.
 

__Висновки__


1. На підставі представлених статистичних даних побудований прототип моделі прогнозування охоплення аудиторії на основі кількості лайків `likes`. Ця залежність адекватно описується простою лінійною залежністю (див. вище) і дозволяє зробити точковий та інтервальний прогноз із надійністю $95\%$ (імовірністю $0,95$) охоплення аудиторії. Включення двох інших параметрів `comments` та `shares` у прогнозну модель недоцільно -- якість моделі не покращується, а точність прогнозу погіршується.

1. Ряд запропонованих альтернативних нелінійних моделей не дав суттєвого покращення якості залежності. Хоча є підстави вважати, що залежність охоплення аудиторії носить нелінійний характер від кількості реакції користувачів мережі, зокрема, кількості лайків, що проявляється на великих значеннях незалежної змінної (змінних). Наявні дані не дозволяють ствердно відповісти на це запитання.

1. Доцільно збільшення об'єму вибіркових даних хоча б на порядок для перевірки адекватності та можливого перенавчання отриманої моделі або побудови складніших залежностей.

1. Головний висновок -- у будь-якому разі перспективи хороші через сильні кореляції між реакціями користувачів та охопленням аудиторії.


Як додатковий приклад та для дослідження можливого впливу на відгук виключених з розгляду предикторів `comments` та `shares` на нових даних до розгляду пропонується дві наступні моделі, що враховують вплив усіх трьох змінних (доцільність виключення констант обґрунтована звіті вище):

* Модель з виключеною константою: $y=b_1x_1+b_2x_2+b_3x_3$

* Узагальнена модель регресії з ефектами взаємодії другого порядку з виключеною константою: $y=b_1x_1+b_2x_2+b_3x_3+b_{12}x_1x_2+b_{13}x_1x_3+b_{23}x_2$ вважається невдалою з точки зору задачі екстраполяції, тобто власне прогнозування.



Будуємо першу модель:

```{r}
lm.reach <- lm(reach ~ comments + likes + shares  - 1, data = dataFilterThreeSigma)
summary(lm.reach)
# plot(lm.reach)
```

Як аналізувати отриману таблицю щодо нових даних?

1. Дивимося на `Coefficients`: якщо бачимо у стовпчику $Pr(>|t|)$ значення __більше, ніж $0,05$__, то коефіцієнт, якому воно відповідає, до моделі __не включається__ -- такий коефіцієнт є статистично незначимим із заданою наперед надійністю $\alpha=0,05$. Тобто, __формально__ модель має вигляд: $y=$ `r summary(lm.reach)$coefficients[1]` $\cdot x_1+$ `r summary(lm.reach)$coefficients[2]` $\cdot x_2+$ `r summary(lm.reach)$coefficients[3]` $\cdot x_3$, але фактично (у цьому конкретному випадку!): $y=$ `r summary(lm.reach)$coefficients[2]` $\cdot x_2$.

Для прогнозування чи перевірки якості прогнозу на тестових даних слід брати лише фактичну модель. Незначимі коефіцієнти повинні вважатися рівними нулю та їх включення в модель лише погіршує точність прогнозу, вносячи як випадкову, так і систематичну похибку.

1. Дивимося оцінку коефіцієнта детермінації: `Multiple R-squared`, тобто який відсоток варіації відгуку пояснює ця модель.
У нашому випадку це: $R^2=$ `r summary(lm.reach)$r.squared`. Тобто модель здатна пояснити варіацію відгуку на `r summary(lm.reach)$r.squared*100` $\%$, кажучи простими словами, "модель хороша" на стільки ж відсотків.

Будуємо другу модель.


```{r}
lm.general.reach <- lm(reach ~ comments + likes + shares + comments*likes + comments*shares + likes*shares - 1, data = dataFilterThreeSigma)
summary(lm.general.reach)
# plot(lm.reach)
```

Як аналізувати отриману таблицю щодо нових даних?

1. Дивимось на `Coefficients`: якщо бачимо у стовпчику $Pr(>|t|)$ значення __більше, ніж $0,05$__, то коефіцієнт, якому воно відповідає, у модель __не включається__ -- __такий коефіцієнт статистично незначимий__ із заданою наперед надійдністю $\alpha=0,05$. Тобто, __формально__ модель має вигляд: $y=$ `r summary(lm.general.reach)$coefficients[1]` $\cdot x_1+$ `r summary(lm.general.reach)$coefficients[2]` $\cdot x_2+$ `r summary(lm.general.reach)$coefficients[3]` $\cdot x_3+$ `r summary(lm.general.reach)$coefficients[4]` $\cdot x_1 x_2+$ `r summary(lm.general.reach)$coefficients[5]` $\cdot x_1 x_3+$ `r summary(lm.general.reach)$coefficients[6]` $\cdot x_2 x_3$,
але фактично (в даному конкретному випадку!): $y=$ `r summary(lm.general.reach)$coefficients[2]` $\cdot x_2$

Для прогнозування чи перевірки якості прогнозу на тестових даних слід брати лише фактичну модель. Незначимі коефіцієнти повинні вважатися рівними нулю та їх включення в модель лише погіршує точність прогнозу, вносячи як випадкову, так і систематичну похибку.

1. Дивимося оцінку коефіцієнта детермінації: `Multiple R-squared`, тобто який відсоток варіації відгуку пояснює дана модель.
У нашому випадку це: $R^2=$ `r summary(lm.general.reach)$r.squared`. Тобто модель здатна пояснити варіацію відгуку на `r summary(lm.general.reach)$r.squared*100` $\%$, кажучи простими словами, "модель хороша" на стільки ж відсотків.


У нашому випадку бачимо, що дві дані моделі звелися до отриманої раніше однофакторної моделі залежності охоплення аудиторії виключно від значень `likes`. При дослідженні нових даних, характер яких відмінний від аналізованих, ситуація може змінитися.


__Побудова моделі `random forest`.__ {#rf}

Як було зазначено вище, параметрична модель дає можливість легкої інтерепретації. Побудва моделі на основі випадкових лісів для даної задачі не є складною процедурою і за своєю проностичною силою не поступається параметричним, хоча і не дозволяє інтерпретувати якісно-кількісні зв'язки, як це дозволяють праметричні моделі.  


```{r}
head(dataFilterThreeSigma)
# 80% даних використовуємо для тренування моделі, 20% для тестування
split <- sample(2, nrow(dataFilterThreeSigma), replace=TRUE, prob=c(0.8, 0.2))
train <- dataFilterThreeSigma[split==1,]
test <- dataFilterThreeSigma[split==2,]

# Побудова моделі
# rf <- randomForest(reach ~ ., data = train)
rf <- randomForest(reach ~ ., data = train, scale=FALSE, ntree=500)
rf
# Побудова прогнозу на тестовій вибірці
predictions <- predict(rf, test)

# ПОхибка прогнозу
print(sqrt(sum((as.vector(predictions - test$reach))^2))/length(predictions))

# Графік похибки прогнозу
plot(rf)

# Оцінка важливості предикторів
importance(rf)
```

Як видно з графіка, значення кількості дерев на рівні $300$ цілком достатньо для мінімізації похибки. Також за результатами оцінки важливості предикторів видно, що `likes` є найбільш значимим з них.


### Індивідуальні завдання на лабораторну роботу

Видає викладач.
