# Модуль 2. Моделювання. Лабораторна робота №4. Побудова регресійних моделей


__Мета:__ _Засвоєння базових принципів, знайомство з інструментами та набуття навичок побудови моделей регресії_ __на рівні технології__ на основи статистичного підходу та моделей машинного навчання засобами мови програмування R та колекції пакетів `dplyr`, `ggplot2`. 


## Що ви будете вміти?

* будувати моделі парної і багатовимірної лінійної та нелінійної регресії на основі статистичних моделей та моделей машинного навчання засобами мови R у середовищі IDE RStudio.
<!-- * обробляти пропущені значення та приводити дані до "охайного" вигляду за допомогою пакету `tidyr`. -->
<!-- * маніпулювати даними засобами мови R у середовищі IDE RStudio в парадигмі пакету `dplyr` з використанням потокового оператору `%>%`. -->


## Короткі теоретичні відомості

У рамках життєвого циклу процесу Data Mining згідно з методологією CRISP DM [@CRISP_DM], наступною за  фазою "Підготовка даних" (__Data Preparing__) є фаза "Моделювання" (__Modelling__) (рис. 1).     
Фаза моделювання призначена для вибору оптимального методу побудови моделей і настроювання його параметрів для отримання оптимальних рішень. На даній фазі вирішуються наступні задачі:  
- вибір методу моделювання;  
- генерація тестового проекту;  
- створення моделей;  
- оцінка моделей.   
Згідно з класичним розумінням технологія Data Mining передбачає побудову моделей, які можна віднести до одного з п'яти основних класів: _кластеризація (сегментація) та аналіз відхилень, регресія, класифікація, пошук асоціативних правил та аналіз послідовних шаблонів_.  

![_Рис. 1. Задача моделювання у складі Data Science-проекту_](image/data-science-model.png)  


Задача сегментації та аналізу відхилень розглядалася нами як складова розвідувального аналізу даних (лаб. роб. №3) у рамках базового модулю.  
У рамках даного модулю буде розглянуто решту моделей. Дана лабораторна робота присвячена регресійному аналізу. Побудова моделей класифікації розглядатиметься у наступній лабораторній роботі.


### Що таке регресія і регресійний аналіз?

__Регресія і класифікація__

Між задачею класифікації і регресії існує багато спільного і в самому загальному вигляді їх можна розглядати як одну. Вона може бути сформульована наступим чином: припустимо, що об'єкт, який нас цікавить, описується вектором $n$ незалежних змінних $X_1, X_2, \dots, X_n$, які називаються предикторами. Існує деяка величина $Y$, яка також характеризує досліджуваний об'єкт, але залежить від $X_1, X_2, \dots, X_n$. Ми маємо колекцію наборів  спостережень незалежних змінних у вигляді матриці $X$ та залежної змінної у вигляді вектора відгуків $Y$: 

$$X=\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1,n} \\
x_{21} & x_{22} & \cdots & x_{2,n} \\
\vdots & \vdots & \ddots & \cdots \\
x_{n1} & x_{n2} & \cdots & x_{n,n} \\
\end{bmatrix},$$  
та 
$$Y=\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}.$$  

На основі інформації, яку несуть в собі наявні значення $X$ та $Y$ необхідно побудувати модель, яка буде передбачати значення відгуку $y^*$ для будь-якого заданого набору незалежних змінних $x^*_1, x^*_2, \dots, x^*_n$  
Відмінність полягає у наступному (рис. 1): якщо значення відгуку носять дискретний характер, говорять про задачу _класифікації_, якщо неперервний -- має місце задача _регресії_.

![Рис. 1. Ілюстрація задач класифікації і регресії [@Paklin]](image/regression_classification.png) 

Таким чином логічно припустити, що і з точки зору математичного апарату і алгоритмів, що його реалізують при розв'язанні задач регресії і класифікації, має бути багато спільного, але, звичайно, мають бути і відмінності.  
У даній лабораторні роботі розглядаються методи і засоби регресійного аналізу.


### Формальна постановка задачі регресії

З точки зору _параметричного підходу_ задача регресії полягає у побудові моделі функціональної залежності математичного сподівання відгуку $Y$ за допомогою невідомої функції регресії $f(\dots)$ з використанням навчальної вибірки:
$$E(Y|x_1, x_2, ..., x_n)=f(
\beta, x_1, x_2, ..., x_n) + \epsilon,$$
де залишки $\epsilon$ відображають похибку моделі, тобто непояснену випадкову варіацію спостережуваних значень залежної змінної відносно очікуваного середнього значення. Такий підхід називається _статистичним_.  
Однак далеко не завжди можливо підібрати адекватну функцію регресії за умови великої кількості предикторів та(або) за умови складного характеру самої регресійної залежності. Тому на даний час добре вивчені і широко поширені моделі регресії на основі _машинного навчання_, які дозволяють розв'язувати цю проблему. 
У роботі [@Fernandes] автори дослідили широке коло існуючих моделей класифікації і зробили висновок стосовно чьотирьох класів моделей, які мають найбільшу точність:  
* __Випадковий ліс” (Random Forest)__;  
* __Машини опорних векторів (Support Vector Machines)__;  
* __Штучні нейронні мережі (Artificial Neural Networks)__;  
* __Бустінгові ансамблі моделей (Boosting Ensembles)__.  
Однак [автори](https://ranalytics.github.io/data-mining/01-Data-Mining-Models-in-R.html#section_1_1) слушно зауважують, що, "перелічені методи практично непридатні для інтерпретації механізмів явища, яке прогнозується, що викликало ряд критичних зауважень".  На підтвердження даної думки можна навести той [факт](https://r-analytics.blogspot.com/2019/09/enterprise-applications-of-r-language.html), що тема [моделей, що можуть бути пояснені (Explainable artificial intelligence)](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence) наразі є найгорячишою темою дискусій у професійному середовищі аналітиків даних.  
Досвід показує, що в тих випадках, коли кількість предикторів є невеликою і залежність є незанадто складною, параметричні методи дозволяють побудувати адекватну модель з високою прогностичною силою, яка одночасно дозволяє і легку інтерпретацію поведінки об'єкта, що вивчається.  
Нижче розглядається статистичний підхід на основі _методу найменших кравдратів (МНК)_ і підхід на основі машинного навчання на прикладі _випадкових лісів._  


### Статистичний підхід

Будемо використовувати матричний підхід.  
Модель лінійної за параметрами регресії у матричній формі має вигляд:

$$Y=X\beta+\epsilon.$$

На практиці ми шукаємо оцінку даного рівнняння:

$$\hat{Y}=Xb,$$

де $b=\begin{bmatrix}
b_0 \\
b_1 \\
\vdots \\
b_k
\end{bmatrix}$. 

На практиці маємо матрицю спостережень предикторів $X$ та вектор відгуків $Y$. Введемо вектор залишків $\epsilon$: 

$$\epsilon=\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots \\
\epsilon_n\\
\end{bmatrix},$$

де $$\epsilon=Y - \hat{Y} = Y - Xb.$$
Відомо, що суть МНК полягає в отриманні оцінок вектора $b$ з умови мінімізації суми квадратів залишків:
$$U(b) = \epsilon^T \epsilon = (Y - Xb)^T (Y - Xb) \rightarrow min. $$
Виконавши диференціювання по параметру $b$ і прирівнявши похідну до нуля, отримаємо нормальні рівння:

$$X^TXb = X^TY,$$

розв'язок яких відносно параметра $b$ дає нам МНК-оцінку вектору коеффіцієнтів моделі $\beta$:

$$b=(X^TX)^{-1}X^TY.$$
 
__Приклад.__ Припустимо, що випадкова величина $X$ -- зріст дорослої людини (см), а $Y$ -- вага її тіла (кг). Ми виконали $n=4$ спостереженнь зросту і ваги випадкових перехожих і хочемо побудувати модель парної лінійної регресії $y=\beta_0 + \beta_1 + \epsilon.$ Для цьго необхідно знайти оцінку рівняння регресії $\hat y = b_0 + b_1x$, попередньо знайшовши МНК-оцінку вектору коефіцієнтів $b \sim \beta$. Запишемо наші дані у матричному вигляді:
$$X=\begin{bmatrix}
1 & 155 \\
1 & 198 \\
1 & 164 \\
1 & 178 \\
\end{bmatrix}, Y=\begin{bmatrix}
60 \\
101 \\
61 \\
85 \\
\end{bmatrix}.$$



Важливо зауважити, що для реалізації МНК у матричній формі матриця $X$ має бути модифікована шляхом додавання одиничного стовпчика зліва.  

Підставимо наші дані у формулу МНК-оцінки: $$b=(X^TX)^{-1}X^TY= 
\Bigg( \begin{bmatrix}
1 & 1 & 1 & 1 \\
155 & 198 & 164 & 178 \\
\end{bmatrix}
\begin{bmatrix}
1 & 155 \\
1 & 198 \\
1 & 164 \\
1 & 178 \\
\end{bmatrix}\Bigg )^{-1}
\begin{bmatrix}
1 & 1 & 1 & 1 \\
155 & 198 & 164 & 178 \\
\end{bmatrix}
\begin{bmatrix}
60 \\
101 \\
61 \\
85 \\
\end{bmatrix} =
\begin{bmatrix}
-103.272\\
1.036
\end{bmatrix}
$$ 
 
Засобами пакета `Matrix` можна провести аналогфчні обчислення і отримати аналогічний результат. 

```{r}
library(Matrix)
X <- matrix(c(1, 155, 1, 198, 1, 164, 1, 178), nrow = 4, ncol = 2, byrow = TRUE)
Y <- c(60, 101, 61, 85)
b <- solve((t(X) %*% X)) %*% t(X) %*% Y
b

```
Побудуємо лінію регресії, скориставшись стандартними засобами мови R.

```{r}
plot(X[, 2], Y,
     main = "Лінія регресії",
     xlab = "x, см",
     ylab = "y, кг"
     )
abline(a = b[1], b = b[2], col = "blue")

```

З графіка видно, що лінія регресії адекватно відображає динаміку "хмари" точок. Однак наступним і важливим етапом є перевірка адекватності моделі.
У середовищі R існує велика кількість спеціалізованих засобів, які дозволяють реалізувати різні статистичні методи побудови моделей регресії, зокрема на основі МНК, і перевірити їх адекватність (див. [Приклад виконання індивідуального завдання](#ind)).  



### Підхід на основі машинного навчання


Under konstruction


## Приклад виконання індивідуального завдання {#ind}

__Задача__: оцінити охоплення аудиторії у Facebook за відомими показниками likes/shares/comments


### Розуміння даних


__Первиниий збір даних__


Маємо [набір даних](data/data.xls), котрий містить вибірку об''мe $n=99$ статистичних даних залежності охоплення (розміру) аудиторії публічного акаунта і реакціями аудиторії на конкретні публікації (табл. 1).


__Опис даних__


Таблиця 1. __Структура початкових статистичних даних__

Характеристика  | Позначення, тип | Кодовое значення 
------------- | ------------- | -------------
номер публікації (Первинний ключ)  | $№$, ціле | `posts`
кількість коментарів до даної публікаії  | $x_1$, ціле | `comments`
кількість лайків до даної публікації  | $x_2$, ціле | `likes`
кількість перепостів до даної публікації  | $x_3$, ціле | `shares`
сумарна реакція  | $x_{123}=\sum_\limits{i=1}^3x_i$, ціле | `all reactions`
охоплення аудиторії  | $y$, ціле | `reach`

 
__Изучение данных и проверка качества данных__

 
```{r, include=FALSE}
# Инсталлируем необходимые пакеты
x <- c("dplyr","rio", "ggplot2", "tidyr", "corrplot", "rio", "PerformanceAnalytics", "FactoMineR", "factoextra", "caret", "GGally", "randomForest")
# if (any(is.na(match(x,installed.packages())))) {
#   install.packages(x)
#   lapply(x, library, character.only = TRUE)
# } else {
#   lapply(x, library, character.only = TRUE)
# }

# install.packages(x)
lapply(x, library, character.only = TRUE)
```


```{r, include=FALSE}
# Импортируем данные
library(magrittr)
library(rio)
y <- c("data/data.xls")
data <- import("data/data.xls")
# View(data)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(
  head(data),
  caption = "Таблица исходных данных"
)
```

Кількість даних __є малим для побудови адекватної прогнозної моделі__, однак, як буде показано нижче, зважаючи на наявність високої кореляції між предикторами і відгуком може бути запропонований до розгляду ряд моделей,які показують обнадійливі результати і на наявних статистичних даних і в перспективі можуть бути перенавчені на нових даних.


### Підготовка даних

 

Дані комплектні, не вимагають траснформаціі і дозволяють перейти безпосередньо до фази побудови моделі.


### Моделювання


> "Не следует множить сущее без необходимости" (William of Ockham)



__Вибір методу моделювання__

Побудова моделі прогнозування охоплення аудиторії являє собою задачу регресійного аналізу, яку в термінах прийнятих позначень (див. Табл. 1) формально можна записати в такий спосіб: необхідно знайти оцінку функції (параметричних або непараметрическим способом) $y = f (x_1, x_2, x_3)$, що дозволяє прогнозувати значення кількісної змінної $y$ від набору незалежних змінних $x_1, x_2, x_3$.

З огляду на, що незалежні змінні і відгук мають числову природу, доцільно в якості основи параметричного підходу взяти класичну статистичну модель багатовимірної лінійної регресії на основі методу найменших квадратів (МНК). Підстави -- легка інтерпретація коефіцієнтів моделі.

Є підстави припускати, що в рамках даного завдання в перспективі дані можуть мати сегментированную структуру, організовувати гомогенні групи, тому доцільно пошукати альтернативний варіант серед непараметричних моделей на основі машинного навчання. Наприклад, на основі нейромереж або random forest. Для такого роду ситуацій найкращим чином (за даними літературних джерел і, зокрема, особистого досвіду автора) підходить модель регресії на основі random forest, яка, на відміну, наприклад, від нейромереж і SVM-моделей добре працює __без попередньої сегментаціі__ вибіркових даних.

З урахуванням вищесказаного і на підставі [результатів] (documents / recognize.rtf) розвідувального аналізу даних, виконаного в середовищі Statgraphics (статпроект [тут] (documents / recognize.sgp)), для вирішення завдання можна запропонувати до розгляду наступні моделі (в першому наближенні):

* Модель множинної лінійної регресії (multiple regression) на основі МНК (Ordinary Least Squares, OLS): $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3)$; в разі необхідності зниження розмірності - покрокова регресія або регресія на головні компоненти (Principal Component Regression);

* Модель парної (Simple regression) лінеарізуемой нелінійної регресії на основі МНК (Squared-Y): $y = \sqrt {\beta_0 + \beta_1x_2}$;

* модель парной линеаризуемой нелинейной регресии на основе МНК (Squared-Y): $y=\sqrt{\beta_0+\beta_1x_{123}}$;

* Модель множинної регресії $y = f (x_1, x_2, x_3)$ на основі випадкового лісу ($random \; forest$).


__Генерація тестового проекту, створення моделей і їх оценка__


Досліджуємо таблицю багатовимірних вибіркових даних, обчисливши оцінку коефіцієнта кореляції Пірсона і побудувавши кореляційні поля (див.нижче).

```{r}
data %>%
  select(comments:shares, `all reactions`, reach) %>% 
  cor() %>% 
  knitr::kable(caption = "Таблица оценок коэффициентов корреляции")
```


```{r}
data %>%
  select(comments:shares, reach) %>% 
ggpairs()
```

```{r, include=FALSE}
CorrMatrix <- data %>% 
  select(comments:shares, `all reactions`, reach) %>% 
  cor()
```


Що ми бачимо? 

1. Відгук `reach` має позитивну кореляцію середньої степені з кажною з трьох вхідних змінних:
    + `comments`: `r CorrMatrix[5, 1]`  
    + `likes`: `r CorrMatrix[5, 2]`
    + `shares`: `r CorrMatrix[5, 3]`
1. Всі три незалежні змінні мають середню і високу ступінь кореляції між собою, що говорить про наявність мультиколінеарності:
    + `comments`-`likes`: `r CorrMatrix[1, 2]`  
    + `comments`-`shares`: `r CorrMatrix[1, 3]`
    + `likes`-`shares`: `r CorrMatrix[2, 3]`

1. Розподілу незалежних змінних і відгуку мають позитивну асиметрію, про що говорить наявність правого хвоста.

1. Дані мають викиди (outliers), які одночасно є і впливовими точками (influential points), тобто виключення або включення їх в модель істотно впливає на її параметри.

Що це означає?

1. Є підстави вважати, що змінні `comments`,` likes`, `shares` впливають на охоплення аудиторії` reach` і можуть виступати в якості незалежних змінних при побудові прогнозної моделі. (Це добре)
1. Наявність мультиколінеарності говорить про те, що лінійна модель прогнозування $ y = f (x_1, x_2, x_3) $ буде неадекватною, при цьому система незалежних змінних надлишкова і вимагає застосування процедур щодо зниження розмірності. (Це не дуже добре)
1. Позитивна асиметрія розподілів змінних викликана наявністю викидів вправо. Це ще один факт на користь того, що лінійні моделі регресії можуть бути неадекватні. Якщо таке трапиться, то вихід може бути, наприклад, таким - вирівнювання розподілів за допомогою логарифмічних або статечних функцій з подальшим застосуванням лінійного МНК (лінеаризація моделі). (Це не дуже добре)
1. Наявність впливових точок - серйозна проблема, особливо в нашому випадку - вибірка мала і тому при проведенні семплірованія для перевірки стійкості оцінок коефіцієнтів моделей це може створювати проблеми. (Це погано) Тим не менш, є міркування, що деякі впливові точки доцільно включати в модель (див. Нижче).



__Модель множественной линейной регресии (multiple regression) на основе МНК (Ordinary Least Squares, OLS): $y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3)$ с пошаговой процедурой__



Построим модель линейной регресии дя всех точек. Видно, что точки с номерами $40$ и $94$ не вписываются в общую картину, поэтому мы их исключаем и проводим повторную оценку коэффициентов модели.


```{r}
dataNotFilter <- select(data, comments:shares, reach)
lm.reach <- lm(reach ~ ., data = dataNotFilter)
summary(lm.reach)
plot(lm.reach)
```

Исключаем выбросы и повторно проводим построение модели регресии.
 
```{r}
dataFilter <- data %>%
  filter(posts %in% c(40, 94) != TRUE) %>% 
  select(comments:shares, reach)

lm.reach <- lm(reach ~ ., data = dataFilter)
summary(lm.reach)
plot(lm.reach)
```

Результаты регрессионного анализа показывают следующее:

1. Регрессия имеет место с коэффициентом детерминации $R^2=$ `r summary(lm.reach)$r.squared`. То есть модель способна объяснить изменчивость отклика на `r summary(lm.reach)$r.squared*100` $\%$, говоря простыми словами, модель "хороша" на столько же процентов.
1. Все коэффициенты, кроме `likes` являются незначимыми, что говорит о наличии мультиколлинеарности и о том, что из трех независимых переменных именно количество лайков наиболее сильно коррелирует с откликом. Значимость свободного члена ($b_0$) на пределе, что намекает на необходимость попытки построить модель без константы.

Выход -- снижение размерности и повторное построение модели.
Для снижения размерности модели и, соответственно, устранении мультиколлинеарности, воспользуемся пошаговой процедурой включения с исключением слабых предикторов ( _Forward Stepwise Selection_ ).



```{r}
lmStep.reach <- step(lm.reach, trace = 0)
summary(lmStep.reach)
```

Видно, что в результате пошаговой регресии мы получили простую модель парной регресии в виде $y=f(x_2)$ практически без уменьшения ее точности, что и подтверждает сравнительный дисперсионный анализ двух моделей -- $y=b_0+b_1x_1+b_2x_2+b_3x_3$ и $y=b_0+b_1x_2$:

```{r}
anova(lm.reach, lmStep.reach)
```

Выполним тестирование двух моделей с использованием десятикратной перекрестной проверки (cross validation).

```{r, warning=FALSE}
lm.reach.cv <- train(reach ~ ., data = dataFilter, method = 'lm', trainControl = trainControl(method = "cv"))
```

```{r, warning=FALSE}
lmStep.reach.cv <- train(reach ~ likes, data = dataFilter, method = 'lm', trainControl = trainControl(method = "cv"))
```


```{r}
lm.reach.cv
lmStep.reach.cv
```

Как видно из результатов простая модель линейной регресии имеет меньшую ошибку (RMSE), чем модель множественной регресии, хотя наблюдается небольшое снижение (на 2%) коэффициента детерминации у второй модели. Но, как показывает проведенный выше дисперсионный анализ, это незначимо. Данную модель можно улучшить, исключив константу из спецификации модели, то есть получить модель в виде $y=b_1x_2$:


```{r}
lm.MinusConst.reach <- lm(reach ~ likes - 1, data = dataFilter)
summary(lm.MinusConst.reach)
plot(lm.MinusConst.reach)
```


```{r}
ggplot(dataFilter,
       aes(x = likes - 1, y = reach,
           colour = comments)) +
  labs(title = "Зависимость охвата аудитории от количества лайков",
       subtitle = "Линейная регрессия с 95%-ными доверительными границами",
       caption = "Без корректировки. Цветом выделено количество комеентариев", 
       x = "Количество лайков", y = "Охват аудитории, кол. чел.") +
  geom_point() +
  stat_smooth(method=lm, se = TRUE, fullrange = TRUE) 

```



Исходя их правила "Три сигма", для корректировки линейной модели целесообразно удаление еще двух точек (11, 24).
  

```{r}
dataFilterThreeSigma <- data %>%
  filter(posts %in% c(11, 24, 40, 94) != TRUE) %>% # указываем номера точек, которые должны быть исключены
  select(comments:shares, reach)
```

Строим модель.

```{r}
lm.MinusConstThreeSigma.reach <- lm(reach ~ likes - 1, data = dataFilterThreeSigma)
summary(lm.MinusConstThreeSigma.reach)
# plot(lm.MinusConst.reach)
```

Строим график.

```{r}
ggplot(dataFilterThreeSigma,
       aes(x = likes - 1, y = reach,
           colour = comments)) +
  labs(title = "Зависимость охвата аудитории от количества лайков",
       subtitle = "Линейная регрессия с 95%-ными доверительными границами",
       caption = "С корректировкой. Цветом выделено количество комментариев", 
       x = "Количество лайков", y = "Охват аудитории, кол. чел.") +
  geom_point() +
  stat_smooth(method=lm, se = TRUE, fullrange = TRUE) 
```



Что имеем и как с этим работать?

1. Уравнение модели: $y=$ `r summary(lm.MinusConstThreeSigma.reach)$coefficients[1]` $\cdot x_2$
Имеем предельно простую и легко интерпретируемую модель: среди двух публикаций, у одной из которых всего лишь на один лайк больше, в среднем на `r floor(summary(lm.MinusConstThreeSigma.reach)$coefficients[1])` просмотров больше аудитория.

1. Регрессия имеет место с коэффициентом детерминации $R^2=$ `r summary(lm.MinusConstThreeSigma.reach)$r.squared`. То есть модель способна объяснить изменчивость отклика на `r summary(lm.MinusConstThreeSigma.reach)$r.squared*100` $\%$, говоря простыми словами, модель "хороша" на столько же процентов.
1. Серая зона на графике показывает надежную зону регрессии -- нижнюю и верхнюю $95\%$-юю границу прогноза для среднего количества лайков. Считается она так: 


```{r}
likesNumber <- data.frame(likes=c(200, 250, 255)) # указываем значения likes для вычисления прогноза по reaches
pre <- predict(lm.MinusConstThreeSigma.reach, likesNumber, interval="confidence")
knitr::kable(cbind(likesNumber, pre),
             caption = "Точечный и интервальный прогноз охвата аудитории")
```

Например, если публикация набрала $200$ лайков, то модель с надежностью не хуже, чем $95\%$ гарантирует, что охват аудитории в среднем `r floor(pre[1, 1])`, но не менее, чем `r floor(pre[1, 2])`, но и не более, чем `r floor(pre[1, 3])`.



__Об остальных альтернативных моделях__


Относительно заявленных в начале альтернативных нелинейных моделях:

* модель парной (Simple regression) линеаризуемой нелинейной регресии на основе МНК (Squared-Y): $y=\sqrt{\beta_0+\beta_1x_2}$;

* модель парной линеаризуемой нелинейной регресии на основе МНК (Squared-Y): $y=\sqrt{\beta_0+\beta_1x_{123}}$;


Как показали исследования, данные модели несущественно отличаются по точности от предложенной линейной модели с одним предиктором `likes`, поэтому нет никаких оснований выдвигать их в качестве достойной альтернативы.


Адекватную прогнозную модель на основе случайных лесов ($random \; forest$) даже в рабочем варианте на малой выборке меньше 100 строить нецелесообразно. При увеличении данных хотя бы на порядок и при наличии зависимостей, отличных от линейной, данный подход может дать интересные и неплохие результаты. 
 

__Выводы__


1. На основании представленных статистических данных построен прототип модели прогнозирования охвата аудитории на основании количества лайков `likes`. Данная зависимость адекватно описывается простой линейной зависимостью (см. выше) и позволяет сделать точечный и интервальный прогноз с надежностью $95\%$ (вероятностью $0,95$) охвата аудитории. Включение двух других параметров `commens` и `shares` в прогнозную модель нецелесообразно -- качество модели не улучшается, а точность прогноза ухудшается.

1. Ряд предложенных альтернативных нелинейные моделей не дал существенного улучшения качества зависимости. Хотя есть основания считать, что зависимость охвата аудитории носит нелинейных характер от количества реакции пользователей сети, в частности, количества лайков, который проявляется на больших значениях независимой переменной (переменных). Представленные заказчиком данные не позволяют утвердительно ответить на данный вопрос.

1. Целесообразно увеличение объема выборочных данных хотя бы на порядок для проверки адекватности и возможного переобучения полученной модели или построения более сложных зависимостей.

1. Главный вывод -- в любом случае перспективы хорошие ввиду сильных корреляций между реакциями пользователей и охватом аудитории.


__p. s.__

По желанию заказчика для возможности исследования возможного влияния на отклик исключенных из рассмотрения предикторов `comments` и `shares`на новых данных к рассмотрению предлагается две следующие модели, учитывающие влияние всех трех переменных (целесообразность исключения констант обоснована в данном отчете выше):

* Модель с исключенной константой: $y=b_1x_1+b_2x_2+b_3x_3$

* Обобщенная модель регрессии с эффектами взаимодействия второго порядка с исключенной константой (изначально предлагаемая заказчиком): $y=b_1x_1+b_2x_2+b_3x_3+b_{12}x_1x_2+b_{13}x_1x_3+b_{23}x_2x_3$ (такой подход к моделированию считается неудачным с точки зрения задачи экстраполяции, т. е. собственно прогнозирования. Оправдан для задач интерполяции)



Строим первую модель:

```{r}
lm.reach <- lm(reach ~ comments + likes + shares  - 1, data = dataFilterThreeSigma)
summary(lm.reach)
# plot(lm.reach)
```

Как анализировать полученную таблицу при исследовании на новых данных?

1. Смотрим на `Coefficients`: если видим в колонке $Pr(>|t|)$ значение __больше, чем $0,05$__, то о  коэффициент, которому оно соответствует, в модель __не включается__ -- __такой коэффициент статистически незначим__ с заданной наперед надежностью $\alpha=0,05$. Т. е., __формально__ модель имеет вид: $y=$ `r summary(lm.reach)$coefficients[1]` $\cdot x_1+$ `r summary(lm.reach)$coefficients[2]` $\cdot x_2+$ `r summary(lm.reach)$coefficients[3]` $\cdot x_3$,
но фактически (в данном конкретном случае!): $y=$ `r summary(lm.reach)$coefficients[2]` $\cdot x_2$.
Для прогнозирования или проверки качества прогноза на тестовых данных следует брать только фактическую модель. Незначимые коэффициенты должны считаться равными нулю и включение их в модель только ухудшает точность прогноза, внося как случайную, так и систематическую погрешность.
1. Смотрим на оценку коэффициента детерминации: `Multiple R-squared`, т. е. какой процент изменчивости отклика объясняет данная модель.
В нашем случае это: $R^2=$ `r summary(lm.reach)$r.squared`. То есть модель способна объяснить изменчивость отклика на `r summary(lm.reach)$r.squared*100` $\%$, говоря простыми словами, модель "хороша" на столько же процентов.

Строим вторую модель.


```{r}
lm.general.reach <- lm(reach ~ comments + likes + shares + comments*likes + comments*shares + likes*shares - 1, data = dataFilterThreeSigma)
summary(lm.general.reach)
# plot(lm.reach)
```

Как анализировать полученную таблицу при исследовании на новых данных?

1. Смотрим на `Coefficients`: если видим в колонке $Pr(>|t|)$ значение __больше, чем $0,05$__, то о  коэффициент, которому оно соответствует, в модель __не включается__ -- __такой коэффициент статистически незначим__ с заданной наперед надежностью $\alpha=0,05$. Т. е., __формально__ модель имеет вид: $y=$ `r summary(lm.general.reach)$coefficients[1]` $\cdot x_1+$ `r summary(lm.general.reach)$coefficients[2]` $\cdot x_2+$ `r summary(lm.general.reach)$coefficients[3]` $\cdot x_3+$ `r summary(lm.general.reach)$coefficients[4]` $\cdot x_1 x_2+$ `r summary(lm.general.reach)$coefficients[5]` $\cdot x_1 x_3+$ `r summary(lm.general.reach)$coefficients[6]` $\cdot x_2 x_3$,
но фактически (в данном конкретном случае!): $y=$ `r summary(lm.general.reach)$coefficients[2]` $\cdot x_2$.
Для прогнозирования или проверки качества прогноза на тестовых данных следует брать только фактическую модель. Незначимые коэффициенты должны считаться равными нулю и включение их в модель только ухудшает точность прогноза, внося как случайную, так и систематическую погрешность.
1. Смотрим на оценку коэффициента детерминации: `Multiple R-squared`, т. е. какой процент изменчивости отклика объясняет данная модель.
В нашем случае это: $R^2=$ `r summary(lm.general.reach)$r.squared`. То есть модель способна объяснить изменчивость отклика на `r summary(lm.general.reach)$r.squared*100` $\%$, говоря простыми словами, модель "хороша" на столько же процентов.


В нашем случае видим, что две данные модели свелись к полученной ранее однофакторной модели зависимости охвата аудитории исключительно от  значений `likes`. При исследовании новых данных, характер которых будет отличен от рассматриваемых, ситуация может измениться.


__Построение модели `random forest`.__

```{r}
head(dataFilterThreeSigma)
# 80% данных используем для тренировки модели, 20% для тестирования
split <- sample(2, nrow(dataFilterThreeSigma), replace=TRUE, prob=c(0.8, 0.2))
train <- dataFilterThreeSigma[split==1,]
test <- dataFilterThreeSigma[split==2,]

# Построение модели
# rf <- randomForest(reach ~ ., data = train)
rf <- randomForest(reach ~ ., data = train, scale=FALSE, ntree=500)
rf
# Построение прогноза на тестовой выборке
predictions <- predict(rf, test)

# Ошибка прогноза
print(sqrt(sum((as.vector(predictions - test$reach))^2))/length(predictions))

# График ошибки прогноза
plot(rf)

# Оценка важности предикторов
importance(rf)
```



__Зауваження.__^[.Rmd файл для генерації __цього документу__ знаходиться [тут](lab4_OIAD_2019.Rmd)]


### Індивідуальні завдання на лабораторну роботу

Видає викладач.


